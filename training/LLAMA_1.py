import re
import os
import json
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, LlamaForCausalLM, AdamW
import random
"""
è‡ªå®šç¾©è³‡æ–™é›†é¡åˆ¥ï¼šç”¨æ–¼è™•ç†æŒ‡ä»¤è§£æä»»å‹™çš„ç”Ÿæˆå¼æ ¼å¼è³‡æ–™
åŠŸèƒ½ï¼š
  1. å°‡åŸå§‹æŒ‡ä»¤å’Œç›®æ¨™JSONçµ„åˆæˆå®Œæ•´æ–‡æœ¬
  2. ä½¿ç”¨tokenizeré€²è¡Œç·¨ç¢¼
  3. è¨ˆç®—æ¨™ç±¤æ™‚å¿½ç•¥æç¤ºè©éƒ¨åˆ†ï¼ˆåªå°è¼¸å‡ºéƒ¨åˆ†è¨ˆç®—lossï¼‰
"""
class WaferCommandDataset(Dataset):
    def __init__(self, commands, outputs, tokenizer, max_len=128):
        self.tokenizer = tokenizer
        self.max_len = max_len
        
        # å®‰å…¨é…å°æ•¸æ“šï¼ˆæ ¸å¿ƒä¿®å¾©ï¼‰
        self.pairs = []
        for i in range(max(len(commands), len(outputs))):
            cmd = commands[i] if i < len(commands) else ""
            out = outputs[i] if i < len(outputs) else '{"object":"","start":"","end":""}'
            self.pairs.append((cmd, out))

    def __len__(self):
        return len(self.pairs)

    def __getitem__(self, idx):
        command, output = self.pairs[idx]
        
        # ç©ºæŒ‡ä»¤è™•ç†
        if not command:
            return self._create_empty_sample()
        
       
        
        prompt = f"ä¾æ“šèªåºï¼Œç¬¬ä¸€å€‹åœ°é»ç‚ºèµ·é»ï¼ˆstartï¼‰ï¼Œç¬¬äºŒå€‹åœ°é»ç‚ºçµ‚é»ï¼ˆendï¼‰ã€‚æŒ‡ä»¤ï¼š{command}\nè«‹è¼¸å‡ºå°æ‡‰ JSONï¼ˆä¸å¯åŠ è§£é‡‹ï¼‰ï¼š"
        # prompt = f"è§£ææŒ‡ä»¤ï¼š{command}\nè¼¸å‡ºJSONï¼š"


        full_text = prompt + output
        
        encoding = self.tokenizer(
            full_text,
            padding='max_length',
            truncation=True,
            max_length=self.max_len,
            return_tensors='pt',
            #pad_to_multiple_of=8
        )
        
        labels = encoding.input_ids.clone()
        prompt_len = len(self.tokenizer.encode(prompt))
        labels[:, :prompt_len] = -100
        
        return {
            'input_ids': encoding.input_ids[0],
            'attention_mask': encoding.attention_mask[0],
            'labels': labels[0]
        }
    
    def _create_empty_sample(self):
        """å‰µå»ºç©ºæ¨£æœ¬é˜²æ­¢å´©æ½°"""
        empty = torch.zeros(self.max_len, dtype=torch.long)
        return {
            'input_ids': empty,
            'attention_mask': empty,
            'labels': empty
        }
class EarlyStopping:
    def __init__(self, patience=3, min_delta=0.0):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = float('inf')
        self.early_stop = False

    def __call__(self, train_loss):
        if train_loss < self.best_loss - self.min_delta:
            self.best_loss = train_loss
            self.counter = 0
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True    
# def train_model(model, dataloader, epochs=5, lr=1e-6):  
#     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#     model.to(device)
    
#     # å„ªåŒ–å™¨
#     optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)
#     # è¨­å®šæ··åˆç²¾åº¦è¨“ç·´ï¼ˆMixed Precision Trainingï¼‰
#     scaler = torch.cuda.amp.GradScaler(enabled=device.type=='cuda') # åªæœ‰åœ¨ä½ ç”¨ GPU è¨“ç·´æ™‚æ‰æœƒå•Ÿç”¨é€™å€‹åŠŸèƒ½ï¼ŒCPU å‰‡è‡ªå‹•é—œé–‰ã€‚
    
#     for epoch in range(epochs):
#         model.train()
#         total_loss = 0
#         valid_count = 0
        
#         for batch in dataloader:
#             try:
#                 # è·³éç©ºæ‰¹æ¬¡
#                 if batch['input_ids'].sum().item() == 0:
#                     continue
                    
#                 optimizer.zero_grad() # æ­¸é›¶æ‰€æœ‰åƒæ•¸çš„æ¢¯åº¦ï¼Œç‚ºæ–°ä¸€è¼ªåå‘å‚³æ’­åšæº–å‚™
                
#                 # æ··åˆç²¾åº¦å‰å‘å‚³æ’­ï¼Œåœ¨ CPU ä¸Šé€™å€‹å€å¡Šç­‰åŒæ–¼æ™®é€šçš„å‰å‘å‚³æ’­ï¼Œ autocast å…¶å¯¦æ²’å•Ÿç”¨
#                 with torch.autocast(device_type=device.type, enabled=device.type=='cuda'):
#                     outputs = model(
#                         input_ids=batch['input_ids'].to(device),
#                         attention_mask=batch['attention_mask'].to(device),
#                         labels=batch['labels'].to(device)
#                     )
#                     loss = outputs.loss
                
#                 # æª¢æŸ¥ç„¡æ•ˆæå¤±
#                 if torch.isnan(loss) or torch.isinf(loss):
#                     print(f"è­¦å‘Šï¼šè·³éç„¡æ•ˆæå¤±å€¼ {loss.item()}")
#                     continue
                
#                 # æ··åˆç²¾åº¦åå‘å‚³æ’­
#                 scaler.scale(loss).backward()
#                 scaler.unscale_(optimizer)
                
#                 # å¼·æ¢¯åº¦è£å‰ª æ‰€æœ‰åƒæ•¸çš„ã€Œæ¢¯åº¦ã€ç¸½é•·åº¦ï¼ˆL2 normï¼‰ï¼Œå¦‚æœè¶…é 0.5ï¼Œå°±æŠŠå®ƒç¸®å°åˆ° 0.5 ä»¥å…§é˜²æ­¢ã€Œæ¢¯åº¦çˆ†ç‚¸ã€
#                 torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)
                
#                 scaler.step(optimizer)
#                 scaler.update()
                
#                 total_loss += loss.item()
#                 valid_count += 1
                
#             except Exception as e:
#                 print(f"è¨“ç·´éŒ¯èª¤: {str(e)}")
#                 continue
        
#         if valid_count > 0:
#             avg_loss = total_loss / valid_count
#             print(f"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}")
#         else:
#             print(f"Epoch {epoch+1}/{epochs} - ç„¡æœ‰æ•ˆæ‰¹æ¬¡")
            
#     return model

# def train_model(model, dataloader, epochs=20, lr=1e-6, patience=3):
#     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#     model.to(device)
#     optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)
#     scaler = torch.cuda.amp.GradScaler(enabled=device.type=='cuda')
#     early_stopper = EarlyStopping(patience=patience)

#     for epoch in range(epochs):
#         model.train()
#         total_loss = 0
#         valid_count = 0
#         for batch in dataloader:
#             try:
#                 if batch['input_ids'].sum().item() == 0:
#                     continue
#                 optimizer.zero_grad()
#                 with torch.autocast(device_type=device.type, enabled=device.type=='cuda'):
#                     outputs = model(
#                         input_ids=batch['input_ids'].to(device),
#                         attention_mask=batch['attention_mask'].to(device),
#                         labels=batch['labels'].to(device)
#                     )
#                     loss = outputs.loss
#                     if torch.isnan(loss) or torch.isinf(loss):
#                         print(f"è­¦å‘Šï¼šè·³éç„¡æ•ˆæå¤±å€¼ {loss.item()}")
#                         continue
#                     scaler.scale(loss).backward()
#                     scaler.unscale_(optimizer)
#                     torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)
#                     scaler.step(optimizer)
#                     scaler.update()
#                     total_loss += loss.item()
#                     valid_count += 1
#             except Exception as e:
#                 print(f"è¨“ç·´éŒ¯èª¤: {str(e)}")
#                 continue
#         if valid_count > 0:
#             avg_loss = total_loss / valid_count
#             print(f"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}")
#         else:
#             avg_loss = float('inf')
#             print(f"Epoch {epoch+1}/{epochs} - ç„¡æœ‰æ•ˆæ‰¹æ¬¡")
#         # Early Stopping åˆ¤æ–·
#         early_stopper(avg_loss)
#         if early_stopper.early_stop:
#             print(f"è¨“ç·´ loss æœªæ”¹å–„ï¼Œæå‰æ–¼ç¬¬ {epoch+1} epoch çµæŸè¨“ç·´ã€‚")
#             break
#     return model

def train_model2(model, dataloader, epochs=20, lr=2e-5, patience=3, eval_prompts=None, tokenizer=None):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)
    scaler = torch.cuda.amp.GradScaler(enabled=device.type=='cuda')
    early_stopper = EarlyStopping(patience=patience)

    for epoch in range(epochs):
        model.train()
        total_loss = 0
        valid_count = 0
        for batch in dataloader:
            try:
                if batch['input_ids'].sum().item() == 0:
                    continue
                optimizer.zero_grad()
                with torch.autocast(device_type=device.type, enabled=device.type=='cuda'):
                    outputs = model(
                        input_ids=batch['input_ids'].to(device),
                        attention_mask=batch['attention_mask'].to(device),
                        labels=batch['labels'].to(device)
                    )
                    loss = outputs.loss
                    if torch.isnan(loss) or torch.isinf(loss):
                        print(f"è­¦å‘Šï¼šè·³éç„¡æ•ˆæå¤±å€¼ {loss.item()}")
                        continue
                    scaler.scale(loss).backward()
                    scaler.unscale_(optimizer)
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)
                    scaler.step(optimizer)
                    scaler.update()
                    total_loss += loss.item()
                    valid_count += 1
            except Exception as e:
                print(f"è¨“ç·´éŒ¯èª¤: {str(e)}")
                continue
        if valid_count > 0:
            avg_loss = total_loss / valid_count
            print(f"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}")
        else:
            avg_loss = float('inf')
            print(f"Epoch {epoch+1}/{epochs} - ç„¡æœ‰æ•ˆæ‰¹æ¬¡")

        # ğŸ” æ¸¬è©¦æ¨è«–è¼¸å‡ºæ•ˆæœ
        if eval_prompts and tokenizer:
            print("\n[æ¨è«–æ¸¬è©¦ç¯„ä¾‹]")
            model.eval()
            parser = CommandParser(model, tokenizer)
            for cmd in eval_prompts[:4]:  # é¡¯ç¤ºå‰4å¥å³å¯
                try:
                    result = parser.parse(cmd)
                    print(f"  æŒ‡ä»¤ï¼š{cmd}")
                    print(f"  æ¨¡å‹è§£æï¼š{result}")
                except Exception as e:
                    print(f"  âŒè§£æå¤±æ•—ï¼š{e}")
            print("[æ¸¬è©¦çµæŸ]\n")

        # Early Stopping åˆ¤æ–·
        early_stopper(avg_loss)
        if early_stopper.early_stop:
            print(f"è¨“ç·´ loss æœªæ”¹å–„ï¼Œæå‰æ–¼ç¬¬ {epoch+1} epoch çµæŸè¨“ç·´ã€‚")
            break

    return model


"""
æŒ‡ä»¤è§£æå™¨é¡åˆ¥ï¼šä½¿ç”¨å¾®èª¿å¾Œçš„LLaMAæ¨¡å‹è§£æè‡ªç„¶èªè¨€æŒ‡ä»¤
åŠŸèƒ½ï¼š
  1. æ¥æ”¶è‡ªç„¶èªè¨€æŒ‡ä»¤
  2. ä½¿ç”¨æ¨¡å‹ç”ŸæˆJSONæ ¼å¼çš„è§£æçµæœ
  3. è‹¥æ¨¡å‹è¼¸å‡ºç„¡æ³•è§£æï¼Œå•Ÿç”¨å‚™ç”¨æ­£å‰‡è§£æç­–ç•¥
"""
# 3. æŒ‡ä»¤è§£æå™¨
class CommandParser:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer

    def parse(self, command):
        
        prompt = f"ä¾æ“šèªåºï¼Œç¬¬ä¸€å€‹åœ°é»ç‚ºèµ·é»ï¼ˆstartï¼‰ï¼Œç¬¬äºŒå€‹åœ°é»ç‚ºçµ‚é»ï¼ˆendï¼‰ã€‚æŒ‡ä»¤ï¼š{command}\nè«‹è¼¸å‡ºå°æ‡‰ JSONï¼ˆä¸å¯åŠ è§£é‡‹ï¼‰ï¼š"

        # prompt = f" è§£ææŒ‡ä»¤ï¼š{command}\nè¼¸å‡ºJSONï¼š"
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
    
         # æ¨™æº–ç”Ÿæˆ
        output_ids = self.model.generate(
            input_ids=inputs.input_ids, # è¼¸å…¥ prompt çš„ token IDï¼Œæ¨¡å‹æ ¹æ“šé€™äº›å…§å®¹ç”Ÿæˆæ–°æ–‡å­—ã€‚
            attention_mask=inputs.attention_mask,  # æ³¨æ„åŠ›é®ç½©:å› ç‚ºå¥å­é•·åº¦ä¸ä¸€ï¼Œè¦å‘Šè¨´æ¨¡å‹å“ªäº› token æ˜¯æœ‰æ•ˆçš„ï¼Œå“ªäº›åªæ˜¯ã€Œè£œé½Šç”¨çš„ç©ºç™½ï¼ˆpaddingï¼‰ã€
            max_new_tokens= 160,  # æœ€å¤šç”Ÿæˆ 160 å€‹æ–° tokenï¼Œé¿å…ç”Ÿæˆå¤ªé•·
            num_beams=3, # ä½¿ç”¨ beam searchï¼Œä¿ç•™ 3 æ¢å€™é¸è·¯å¾‘ï¼Œèƒ½æ¢ç´¢æ›´å¤šå¯èƒ½æ€§ï¼Œé¿å…åªèµ°å–®ä¸€è·¯å¾‘ã€‚ç”Ÿæˆçš„å¥å­æ›´è‡ªç„¶ã€å¤šæ¨£ã€‚(æ¯ä¸€æ­¥éƒ½åªé¸æ“‡æ©Ÿç‡æœ€å¤§çš„è©ï¼Œçµæœå¸¸å¸¸å–®èª¿ã€é‡è¤‡)
            do_sample=False, # ä¸éš¨æ©Ÿå–æ¨£ï¼Œä¿è­‰ç”Ÿæˆçµæœå¯é‡ç¾   
            pad_token_id=self.tokenizer.eos_token_id, # è£œé½Šè®“ä¸€å€‹ batch è£¡æ‰€æœ‰è¼¸å…¥çš„é•·åº¦ä¸€è‡´ï¼Œæ–¹ä¾¿ä¸¦è¡Œé‹ç®—
            forced_bos_token_id=self.tokenizer.encode("{")[1]  # å¼·åˆ¶JSONé–‹é ­ï¼Œå¼·åˆ¶ç”Ÿæˆçš„ç¬¬ä¸€å€‹ token å¿…é ˆæ˜¯ {
        )
        
        # æå–JSONéƒ¨åˆ†
        # full_output = self.tokenizer.decode(output_ids[0]) # æŠŠæ¨¡å‹ç”¢ç”Ÿçš„ token è½‰å›å®Œæ•´çš„æ–‡å­—
        full_output = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)
        # æŠŠå­—ä¸²ç”¨ "è¼¸å‡ºJSONï¼š" é€™å€‹é—œéµè©åˆ†å‰²ï¼Œå–å‡ºå¾Œé¢æ®µè½ï¼ˆ[-1]ï¼‰ï¼Œé€™æ¨£å¯ä»¥è·³éå‰é¢ä¸æ˜¯ JSON çš„éƒ¨åˆ†ã€‚ å†ç”¨ } åˆ†å‰²ï¼Œå–ç¬¬ä¸€æ®µï¼ˆï¼‰ï¼Œç„¶å¾Œè£œä¸Šä¸€å€‹ }(é¿å…é‡è¤‡è¼¸å‡º)
        # json_str = full_output.split("è¼¸å‡ºJSONï¼š")[-1].split("}")[0] + "}"  
        # å˜—è©¦å¾ full_output ä¸­æ‰¾å‡ºç¬¬ä¸€å€‹ {...} çµæ§‹
        match = re.search(r'\{.*?\}', full_output)
        print("full output:\n")
        print(full_output)
        if match:
            json_str = match.group()
            print(f"\njson_str\n {json_str}")

            return json.loads(json_str)
        else:
            raise ValueError(f"ç„¡æ³•è§£ææŒ‡ä»¤: {command}")


        # json_str = full_output.split("è¼¸å‡ºï¼š")[0].split("}")[0] + "}"
        # print("full output:\n")
        # print(full_output)
        # print(f"json_str\n {json_str}")


        # try:
        #     return json.loads(json_str) # JSON å­—ä¸²è½‰å› Python å­—å…¸
        # except:
        #     raise ValueError(f"ç„¡æ³•è§£ææŒ‡ä»¤: {command}")
            # return self._fallback_parse(command) #å‚™ç”¨æ–¹æ¡ˆ

    def _fallback_parse(self, command):
        print("å•Ÿç”¨å‚™ç”¨æ–¹æ¡ˆ~")
        num_matches = re.findall(r'([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å1234567890]+)ç«™', command)
        object_match = re.search(r'(æ™¶åœ“ç›’|é‡‘å…ƒå’Œ|Wafer)', command)
        object_type = object_match.group(1) if object_match else "æ™¶åœ“ç›’"
        
        if len(num_matches) >= 2:
            return {
                "object": object_type,
                "start": f"ç¬¬{num_matches[0]}ç«™",
                "end": f"ç¬¬{num_matches[1]}ç«™"
            }
        raise ValueError(f"ç„¡æ³•è§£ææŒ‡ä»¤: {command}")
"""
å‹•ä½œåŸ·è¡Œå™¨é¡åˆ¥ï¼šæ ¹æ“šè§£æçµæœåŸ·è¡Œæ©Ÿå™¨äººå‹•ä½œåºåˆ—
å‹•ä½œåºåˆ—ï¼š
  1. å°èˆªè‡³èµ·é»ç«™
  2. åœ¨èµ·é»ç«™æŠ“å–æ™¶åœ“ç›’
  3. å°èˆªè‡³çµ‚é»ç«™
  4. åœ¨çµ‚é»ç«™æ”¾ç½®æ™¶åœ“ç›’
"""
# 4. å‹•ä½œåŸ·è¡Œå™¨ï¼ˆä¿æŒä¸è®Šï¼‰
class ActionExecutor:
    @staticmethod
    def navigate_to(station):
        print(f"[å‹•ä½œ] å°èˆªè‡³ {station}")

    @staticmethod
    def pick_wafer(station):
        print(f"[å‹•ä½œ] åœ¨ {station} æŠ“å–æ™¶åœ“ç›’")

    @staticmethod
    def place_wafer(station):
        print(f"[å‹•ä½œ] åœ¨ {station} æ”¾ç½®æ™¶åœ“ç›’")

    @classmethod
    def execute_sequence(cls, parsed_cmd):
        actions = [
            ("navigate", parsed_cmd["start"]),
            ("pick", parsed_cmd["start"]),
            ("navigate", parsed_cmd["end"]),
            ("place", parsed_cmd["end"])
        ]
        print(f"\né–‹å§‹åŸ·è¡Œ {parsed_cmd['object']} æ¬é‹ä»»å‹™:")
        for action, param in actions:
            if action == "navigate":
                cls.navigate_to(param)
            elif action == "pick":
                cls.pick_wafer(param)
            elif action == "place":
                cls.place_wafer(param)
        print("ä»»å‹™å®Œæˆ!\n")
"""
ä¸»ç¨‹å¼æµç¨‹ï¼š
  1. æº–å‚™è¨“ç·´è³‡æ–™ï¼ˆæŒ‡ä»¤â†’JSONå°ï¼‰
  2. åˆå§‹åŒ–æ¨¡å‹å’Œtokenizer
  3. ä¿®å¾©tokenizerçš„paddingå•é¡Œ
  4. è¨“ç·´ä¸¦ä¿å­˜æ¨¡å‹
  5. æ¸¬è©¦æ¨¡å‹è§£æèƒ½åŠ›
"""
def data_pre():
    train_commands = [
        "è«‹æŠŠæ™¶åœ“ç›’å¾ç¬¬äºŒç«™æ¬åˆ°ç¬¬ä¸‰ç«™",
        "é‡‘å…ƒå’Œå¾ç¬¬ä¸€ç«™æ¬é‹ç¬¬å››ç«™",
        "å°‡Waferç”±äºŒç«™ç§»å‹•è‡³ä¸€ç«™",
        "ç·Šæ€¥ä»»å‹™ï¼šå¾ä¸‰ç«™æ¬æ™¶åœ“ç›’åˆ°ä¸€ç«™",

        "å››ç«™æ™¶åœ“ç›’è½‰é‹åˆ°1ç«™",
        "ç¬¬äºŒç«™åˆ°ç¬¬3ç«™",
        "æŠŠç¬¬1ç«™çš„æ™¶åœ“ç›’æ¬åˆ°ç¬¬äºŒç«™",
        "ç¬¬äºŒç«™æ‹¿åˆ°ç¬¬å››ç«™",

        "å°‡ç¬¬ä¸€çš„Waferç·Šæ€¥è½‰ç§»è‡³ç¬¬äº”",            # æ–°å¢è¤‡é›œæ ¼å¼
        "å¾äºŒè™Ÿç«™å–æ™¶åœ“ç›’é€åˆ°å››è™Ÿç«™",
        "æ¬é‹ä»»å‹™ï¼šèµ·é»=ç¬¬äºŒç«™, çµ‚é»=ç¬¬å…­ç«™, ç‰©é«”=é‡‘å…ƒå’Œ", # æ–°å¢é‚Šç•Œæ¡ˆä¾‹
        "æ™¶åœ“ç›’ç¬¬4ç«™åˆ°äºŒç«™",  # æ•…æ„ç¼ºå°‘é—œéµè©

        "æ™¶åœ“ç›’ç¬¬ã„§ç«™ï¼Œæ‹¿åˆ°ç¬¬å››ç«™",
        "å°‡ç¬¬ä¸‰å€‹ç·Šæ€¥è½‰ç½®ç¬¬äºŒ",
        "å…©å€‹æ™¶åœ“ç›’ç¬¬ä¸€ç«™åˆ°äºŒç«™",
        "å››åˆ°ä¸‰",

        "2åˆ°1",
        "3åˆ°äºŒ",
        "ç¬¬å››ç«™æ‹¿åˆ°ç¬¬ä¸‰ç«™",
        "äºŒç«™è‡³ä¸€ç«™",

        "è«‹å°‡æ™¶åœ“ç›’å¾ç¬¬äº”ç«™æ¬åˆ°ç¬¬å…­ç«™",
        "é‡‘å…ƒå’Œå¾ç¬¬ä¸ƒç«™æ¬é‹ç¬¬äºŒç«™",
        "å°‡Waferç”±ä¸‰ç«™ç§»å‹•è‡³å››ç«™",
        "ç·Šæ€¥ä»»å‹™ï¼šå¾å…­ç«™æ¬æ™¶åœ“ç›’åˆ°äº”ç«™",

        "å…«ç«™æ™¶åœ“ç›’è½‰é‹åˆ°äºŒç«™",
        "ç¬¬äº”ç«™åˆ°ç¬¬å…«ç«™",
        "æŠŠç¬¬ä¸‰ç«™çš„æ™¶åœ“ç›’æ¬åˆ°ç¬¬ä¸€ç«™",
        "å››ç«™æ‹¿åˆ°ç¬¬ä¸€ç«™",

        "å°‡ç¬¬å››çš„Waferç·Šæ€¥è½‰ç§»è‡³ç¬¬äºŒ",
        "å¾ä¸ƒè™Ÿç«™å–æ™¶åœ“ç›’é€åˆ°äº”è™Ÿç«™",
        "æ¬é‹ä»»å‹™ï¼šèµ·é»=ç¬¬ä¸‰ç«™, çµ‚é»=ç¬¬9ç«™, ç‰©é«”=é‡‘å…ƒå’Œ",
        "æ™¶åœ“ç›’ç¬¬ä¸‰ç«™åˆ°äºŒç«™",

        "æ™¶åœ“ç›’ç¬¬å››ç«™ï¼Œæ‹¿åˆ°ç¬¬ä¸‰ç«™",
        "å°‡ç¬¬äº”å€‹ç·Šæ€¥è½‰ç½®ç¬¬å…­",
        "å…©å€‹æ™¶åœ“ç›’ç¬¬ä¹ç«™åˆ°ç¬¬å…«ç«™",
        "ä¸€åˆ°å…­",

        "äº”åˆ°äºŒ",
        "ä¸€åˆ°ä¸‰",
        "ç¬¬ä¸ƒç«™æ‹¿åˆ°ç¬¬å…­ç«™",
        "å››ç«™è‡³äº”ç«™",

        "è«‹æŠŠWaferå¾å…«ç«™è½‰é‹åˆ°ä¸‰ç«™",
        "æ¬é‹ä»»å‹™ï¼šèµ·é»=äºŒç«™, çµ‚é»=å››ç«™, ç‰©é«”=æ™¶åœ“ç›’",
        "é‡‘å…ƒå’Œå¾äº”ç«™é€åˆ°ä¸€ç«™",
        "Waferå¾ä¸€ç«™ç§»å‹•åˆ°äºŒç«™",

        "æ™¶åœ“ç›’å¾ä¸‰ç«™æ¬åˆ°ä¸ƒç«™",
        "ç·Šæ€¥ï¼šå¾å››ç«™æ¬æ™¶åœ“ç›’åˆ°å…«ç«™",
        "äºŒç«™æ™¶åœ“ç›’è½‰é‹åˆ°ä¸€ç«™",
        "ç¬¬ä¸€ç«™åˆ°ç¬¬ä¸‰ç«™",

        "ç¬¬å››ç«™æ‹¿åˆ°ç¬¬ä¸ƒç«™",
        "ä¸€ç«™åˆ°äºŒç«™",
        "è«‹æŠŠæ™¶åœ“ç›’å¾ç¬¬äºŒç«™æ¬é‹åˆ°ç¬¬ä¸€ç«™",
        "ç¬¬ä¸€ç«™æ‹¿åˆ°ç¬¬äºŒç«™",

        "è«‹æŠŠæ™¶åœ“ç›’å¾ç¬¬äºŒç«™æ‹¿åˆ°ç¬¬ä¸€ç«™",
        "è«‹å¹«æˆ‘æŠŠå®ƒå¾ç¬¬äºŒç«™ï¼Œæ¬åˆ°ç¬¬ä¸‰ç«™",
        "ç¬¬ä¸‰ç«™æ¬åˆ°ç¬¬å››ç«™",
        "ç¬¬äºŒç«™ï¼Œæ¬åˆ°ç¬¬ä¸€ç«™",
    ]
    train_outputs = [
        '{"object": "æ™¶åœ“ç›’", "start": "ç¬¬äºŒç«™", "end": "ç¬¬ä¸‰ç«™"}',
        '{"object": "é‡‘å…ƒå’Œ", "start": "ç¬¬ä¸€ç«™", "end": "ç¬¬å››ç«™"}',
        '{"object": "Wafer", "start": "äºŒç«™", "end": "ä¸€ç«™"}',
        '{"object": "æ™¶åœ“ç›’", "start": "ä¸‰ç«™", "end": "ä¸€ç«™"}',

        '{"object": "é‡‘å…ƒå’Œ", "start": "å››ç«™", "end": "1ç«™"}',
        '{"object": "æ™¶åœ“ç›’", "start": "ç¬¬äºŒç«™", "end": "ç¬¬3ç«™"}',
        '{"object": "æ™¶åœ“ç›’", "start": "ç¬¬1ç«™", "end": "ç¬¬äºŒç«™"}',
        '{"object": "æ™¶åœ“ç›’", "start": "ç¬¬äºŒç«™", "end": "ç¬¬å››ç«™"}',
        # å¼·åŒ–JSONæ ¼å¼ç´°ç¯€
        '{"object":"Wafer", "start":"ç¬¬ä¸€", "end":"ç¬¬äº”"}',
        '{"object":"æ™¶åœ“ç›’", "start":"äºŒè™Ÿç«™", "end":"å››è™Ÿç«™"}',
        '{"object": "é‡‘å…ƒå’Œ", "start": "ç¬¬äºŒç«™", "end": "ç¬¬å…­ç«™"}',# æ–°å¢å¸¶å¼•è™Ÿçš„æ¨™æº–æ ¼å¼
        '{"object": "æ™¶åœ“ç›’", "start": "ç¬¬4ç«™", "end": "äºŒç«™"}',

        '{"object": "æ™¶åœ“ç›’", "start": "ç¬¬ä¸€ç«™", "end": "ç¬¬å››ç«™"}',
        '{"object": "æ™¶åœ“ç›’", "start": "ç¬¬ä¸‰", "end": "ç¬¬äºŒ"}',
        '{"object": "æ™¶åœ“ç›’", "start": "ç¬¬ä¸€ç«™", "end": "ç¬¬äºŒç«™"}',
        '{"object": "æ™¶åœ“ç›’", "start": "å››", "end": "ä¸‰"}',

        '{"object": "æ™¶åœ“ç›’", "start": "2", "end": "1"}',
        '{"object": "æ™¶åœ“ç›’", "start": "3", "end": "äºŒ"}',
        '{"object": "æ™¶åœ“ç›’", "start": "ç¬¬å››ç«™", "end": "ç¬¬ä¸‰ç«™"}',
         '{"object": "æ™¶åœ“ç›’", "start": "äºŒç«™", "end": "ä¸€ç«™"}',

         '{"object": "æ™¶åœ“ç›’", "start": "ç¬¬äº”ç«™", "end": "ç¬¬å…­ç«™"}',
        '{"object": "é‡‘å…ƒå’Œ", "start": "ç¬¬ä¸ƒç«™", "end": "ç¬¬äºŒç«™"}',
        '{"object": "Wafer", "start": "ä¸‰ç«™", "end": "å››ç«™"}',
        '{"object": "æ™¶åœ“ç›’", "start": "å…­ç«™", "end": "äº”ç«™"}',

        '{"object": "æ™¶åœ“ç›’", "start": "å…«ç«™", "end": "äºŒç«™"}',
        '{"object": "æ™¶åœ“ç›’", "start": "ç¬¬äº”ç«™", "end": "ç¬¬å…«ç«™"}',
        '{"object": "æ™¶åœ“ç›’", "start": "ç¬¬ä¸‰ç«™", "end": "ç¬¬ä¸€ç«™"}',
        '{"object": "æ™¶åœ“ç›’", "start": "å››ç«™", "end": "ç¬¬ä¸€ç«™"}',

        '{"object":"Wafer", "start":"ç¬¬å››", "end":"ç¬¬äºŒ"}',
        '{"object":"æ™¶åœ“ç›’", "start":"ä¸ƒè™Ÿç«™", "end":"äº”è™Ÿç«™"}',
        '{"object": "é‡‘å…ƒå’Œ", "start": "ç¬¬ä¸‰ç«™", "end": "ç¬¬9ç«™"}',
        '{"object": "æ™¶åœ“ç›’", "start": "ç¬¬ä¸‰ç«™", "end": "äºŒç«™"}',

        '{"object": "æ™¶åœ“ç›’", "start": "ç¬¬å››ç«™", "end": "ç¬¬ä¸‰ç«™"}',
        '{"object": "æ™¶åœ“ç›’", "start": "ç¬¬äº”", "end": "ç¬¬å…­"}',
        '{"object": "æ™¶åœ“ç›’", "start": "ç¬¬ä¹ç«™", "end": "ç¬¬å…«ç«™"}',
        '{"object": "æ™¶åœ“ç›’", "start": "ä¸€", "end": "å…­"}',

        '{"object": "æ™¶åœ“ç›’", "start": "äº”", "end": "äºŒ"}',
        '{"object": "æ™¶åœ“ç›’", "start": "ä¸€", "end": "ä¸‰"}',
        '{"object": "æ™¶åœ“ç›’", "start": "ç¬¬ä¸ƒç«™", "end": "ç¬¬å…­ç«™"}',
        '{"object": "æ™¶åœ“ç›’", "start": "å››ç«™", "end": "äº”ç«™"}',

        '{"object": "Wafer", "start": "å…«ç«™", "end": "ä¸‰ç«™"}',
        '{"object": "æ™¶åœ“ç›’", "start": "äºŒç«™", "end": "å››ç«™"}',
        '{"object": "é‡‘å…ƒå’Œ", "start": "äº”ç«™", "end": "ä¸€ç«™"}',
        '{"object": "Wafer", "start": "ä¸€ç«™", "end": "äºŒç«™"}',

        '{"object": "æ™¶åœ“ç›’", "start": "ä¸‰ç«™", "end": "ä¸ƒç«™"}',
        '{"object": "æ™¶åœ“ç›’", "start": "å››ç«™", "end": "å…«ç«™"}',
        '{"object": "æ™¶åœ“ç›’", "start": "äºŒç«™", "end": "ä¸€ç«™"}',
        '{"object": "æ™¶åœ“ç›’", "start": "ç¬¬ä¸€ç«™", "end": "ç¬¬ä¸‰ç«™"}',

        '{"object": "æ™¶åœ“ç›’", "start": "ç¬¬å››ç«™", "end": "ç¬¬ä¸ƒç«™"}',
        '{"object": "æ™¶åœ“ç›’", "start": "ä¸€ç«™", "end": "äºŒç«™"}',
        '{"object": "æ™¶åœ“ç›’", "start": "ç¬¬äºŒç«™", "end": "ç¬¬ä¸€ç«™"}',
        '{"object": "æ™¶åœ“ç›’", "start": "ç¬¬ä¸€ç«™", "end": "ç¬¬äºŒç«™"}',

        '{"object": "æ™¶åœ“ç›’", "start": "ç¬¬äºŒç«™", "end": "ç¬¬ä¸€ç«™"}',
        '{"object": "æ™¶åœ“ç›’", "start": "ç¬¬äºŒç«™", "end": "ç¬¬ä¸‰ç«™"}',
        '{"object": "æ™¶åœ“ç›’", "start": "ç¬¬ä¸‰ç«™", "end": "ç¬¬å››ç«™"}',
        '{"object": "æ™¶åœ“ç›’", "start": "ç¬¬äºŒç«™", "end": "ç¬¬ä¸€ç«™"}',

    ]
    # å®šç¾©ç‰©ä»¶å’Œç«™é»
    objects = ["æ™¶åœ“ç›’"]
    stations = [f"ç¬¬{i}ç«™" for i in ["ä¸€","äºŒ","ä¸‰","å››","äº”"]]

    # è‡ªå‹•æ“´å¢è³‡æ–™è‡³ 1000 ç­†
    while len(train_commands) < 1000:
        obj = random.choice(objects)
        start = random.choice(stations)
        end = random.choice(stations)
        while end == start:
            end = random.choice(stations)

        phrasing = [
            f"è«‹å°‡{obj}å¾{start}æ¬åˆ°{end}",
            f"{obj}å¾{start}ç§»å‹•è‡³{end}",
            f"æ¬é‹ä»»å‹™ï¼šå¾{start}å–{obj}é€åˆ°{end}",
            f"è«‹æŠŠ{start}çš„{obj}æ¬åˆ°{end}",
            f"{start}æ‹¿åˆ°{end}",
            f"{start}åˆ°{end}",
            f"è«‹å°‡{obj}å¾{start}æ‹¿åˆ°{end}",
            f"å¾{start}æ¬åˆ°{end}",
            
        ]
        cmd = random.choice(phrasing)
        train_commands.append(cmd)
        train_outputs.append(json.dumps({"object": obj, "start": start, "end": end}, ensure_ascii=False))

    len(train_commands), len(train_outputs)
    # å°‡æ“´å¢å¾Œçš„è³‡æ–™å„²å­˜ç‚º JSON æª”æ¡ˆ
    output_data = [{"command": cmd, "output": out} for cmd, out in zip(train_commands, train_outputs)]

    output_path = "augmented_wafer_dataset.json"
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)

# 5. ä¸»ç¨‹å¼ï¼ˆé—œéµä¿®æ”¹ï¼šä¿®å¾©paddingå•é¡Œï¼‰
if __name__ == "__main__":
   
    # data_pre()
    with open("augmented_wafer_dataset.json", "r", encoding="utf-8") as f:
        data = json.load(f)
    # æ‹†åˆ†ç‚ºå…©å€‹ list
    train_commands = [item["command"] for item in data]
    train_outputs = [item["output"] for item in data]
    print(train_commands[0])
    print(train_outputs[0])
    for i, out in enumerate(train_outputs):
        try:
            js = json.loads(out)
            if js["start"] == js["end"]:
                print(f"ç¬¬{i}ç­†èµ·é»èˆ‡çµ‚é»ç›¸åŒï¼š{train_commands[i]}")
        except:
            print(f"JSON éŒ¯èª¤æ ¼å¼ï¼š{out}")
    val_commands = [
        "æŠŠç¬¬ä¸‰ç«™çš„é‡‘å…ƒå’Œæ¬åˆ°ç¬¬å››ç«™",
        "æ™¶åœ“ç›’å¾ç¬¬ä¸€ç«™æ¬é‹ç¬¬äºŒç«™",
        "ç¬¬äºŒç«™æ¬é‹åˆ°ç¬¬ä¸‰ç«™",
        "è«‹æŠŠæ™¶åœ“ç›’å¾ç¬¬ä¸€ç«™æ¬é‹åˆ°ç¬¬å››ç«™"
    ]
    # åˆå§‹åŒ– tokenizer å’Œæ¨¡å‹ 
    tokenizer = AutoTokenizer.from_pretrained("p208p2002/llama-traditional-chinese-120M")
    
    # ä¿®å¾©paddingå•é¡Œï¼šè¨­ç½®pad_token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    model = LlamaForCausalLM.from_pretrained(
        "p208p2002/llama-traditional-chinese-120M",
        pad_token_id=tokenizer.eos_token_id  # æ˜ç¢ºæŒ‡å®špad_token_id
    )

    # å‰µå»ºè³‡æ–™é›†å’Œæ•¸æ“šåŠ è¼‰å™¨
    dataset = WaferCommandDataset(
        commands=train_commands,
        outputs=train_outputs,
        tokenizer=tokenizer,
        max_len=320 #96
    )
    dataloader = DataLoader(dataset, batch_size = 8, shuffle=True)

    # è¨“ç·´æ¨¡å‹
    # trained_model = train_model(model, dataloader, epochs= 15)
    trained_model = train_model2(
        model, 
        dataloader, 
        epochs = 18, 
        patience=3, 
        eval_prompts=val_commands, 
        tokenizer=tokenizer
    )
    # ä¿å­˜æ¨¡å‹ (åŒ…å«tokenizeré…ç½®)
    output_dir = "custom_wafer_llama"
    trained_model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    print(f"æ¨¡å‹è¨“ç·´å®Œæˆä¸¦å·²ä¿å­˜è‡³ {output_dir}")

    # æ¸¬è©¦æŒ‡ä»¤é›†
    # test_commands = [
    #     "æŠŠç¬¬2ç«™çš„é‡‘å…ƒå’Œæ¬åˆ°ç¬¬ä¸‰ç«™",
    #     "æŠŠç¬¬ä¸‰ç«™çš„é‡‘å…ƒå’Œé‹åˆ°ç¬¬1ç«™",
    #     "Waferç”±äº”ç«™æ‹¿åˆ°ç¬¬2ç«™",
    #     "ç·Šæ€¥ä»»å‹™ï¼šå¾ä¸‰ç«™æ¬æ™¶åœ“ç›’åˆ°ä¹ç«™",
    #     "å°‡äºŒç«™æ™¶åœ“ç›’è½‰é‹åˆ°ä¸‰ç«™",
    #     "ç¬¬5ç«™åˆ°ç¬¬ä¸€ç«™",
    #     "æ™¶åœ“ç›’å¾ç¬¬1ç«™æ¬é‹ç¬¬3ç«™",
    #     "è«‹æŠŠæ™¶åœ“ç›’å¾ç¬¬äºŒç«™æ¬é‹åˆ°ç¬¬ä¸‰ç«™"
    # ]
    test_commands = [
        "æŠŠç¬¬äºŒç«™çš„é‡‘å…ƒå’Œæ¬åˆ°ç¬¬ä¸‰ç«™",
        "æ™¶åœ“ç›’å¾ç¬¬1ç«™æ¬é‹ç¬¬3ç«™",
        "è«‹æŠŠæ™¶åœ“ç›’å¾ç¬¬å››ç«™æ¬é‹åˆ°ç¬¬ä¸‰ç«™",
        "ç¬¬äºŒç«™æ¬åˆ°ç¬¬ä¸€ç«™"
    ]

    print("\n=== é–‹å§‹å…¨é¢æ¸¬è©¦ ===")
    parser = CommandParser(trained_model, tokenizer)
    executor = ActionExecutor()

    for i, cmd in enumerate(test_commands, 1):
        print(f"\næ¸¬è©¦ {i}/{len(test_commands)}")
        print(f"è¼¸å…¥æŒ‡ä»¤: ã€Œ{cmd}ã€")
        try:
            parsed_result = parser.parse(cmd)
            print(f"è§£æçµæœ: {parsed_result}")
            executor.execute_sequence(parsed_result)
        except Exception as e:
            print(f"è§£æéŒ¯èª¤: {str(e)}")
    print("\n=== å…¨é¢æ¸¬è©¦å®Œæˆ ===")
